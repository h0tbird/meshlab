#!/usr/bin/env bash

#------------------------------------------------------------------------------
# [i] Initializations
#------------------------------------------------------------------------------

# Bash strict mode
set -euo pipefail

# Change to the execution directory
cd "$(dirname "$0")"/..

# shellcheck source=../lib/common.sh
source ./lib/common.sh

#------------------------------------------------------------------------------
# [i] Versions
#------------------------------------------------------------------------------

KINDCCM_VERSION='0.10.0'                      # https://github.com/kubernetes-sigs/cloud-provider-kind/releases
CILIUM_CHART_VERSION='1.18.5'                 # https://artifacthub.io/packages/helm/cilium/cilium
K8S_GATEWAY_CHART_VERSION='3.2.8'             # https://github.com/k8s-gateway/k8s_gateway/blob/master/charts/k8s-gateway/Chart.yaml
ARGOCD_CHART_VERSION='9.1.9'                  # https://artifacthub.io/packages/helm/argo-cd-oci/argo-cd
ARGOWF_CHART_VERSION='0.46.2'                 # https://artifacthub.io/packages/helm/argo/argo-workflows
PROMETHEUS_CHART_VERSION='27.52.0'            # https://artifacthub.io/packages/helm/prometheus-community/prometheus
GRAFANA_CHART_VERSION='10.4.0'                # https://artifacthub.io/packages/helm/grafana/grafana
OTEL_COLLECTOR_CHART_VERSION='0.142.0'        # https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector
VAULT_CHART_VERSION='0.31.0'                  # https://artifacthub.io/packages/helm/hashicorp/vault
CERT_MANAGER_CHART_VERSION='v1.19.2'          # https://artifacthub.io/packages/helm/cert-manager/cert-manager
KUBERNETES_REPLICATOR_CHART_VERSION='2.12.2'  # https://artifacthub.io/packages/helm/kubernetes-replicator/kubernetes-replicator
ISTIO_CHART_VERSION='1.28.2'                  # https://artifacthub.io/packages/helm/istio-official/base
KIALI_CHART_VERSION='2.19.0'                  # https://artifacthub.io/packages/helm/kiali/kiali-operator

#------------------------------------------------------------------------------
# [i] Validate the command line arguments
#------------------------------------------------------------------------------

[[ $# -lt 1 || ($1 != "create" && $1 != "delete") ]] && \
{ echo "Usage: $(basename "$0") <create|delete> [1|2]"; exit 1; }
WLCNT="${3:-1}" # Number of workload cells to create (default 1)

#------------------------------------------------------------------------------
# [i] Handle the delete command
#------------------------------------------------------------------------------

if [[ $1 == "delete" ]]; then

  blue "───[ Cleanup ]────────────────────────────────────────────────────────"

  # shellcheck disable=SC2046
  docker kill $(docker ps -f "name=kindccm" -q) 2>/dev/null || true
  argocd context --delete "${MNGR}" 2>/dev/null || true
  kind delete clusters --all
  rm -rf ./tmp/*
  exit 0
fi

#------------------------------------------------------------------------------
# [i] cloud-provider-kind
#------------------------------------------------------------------------------

blue "───[ cloud-provider-kind ]──────────────────────────────────────────────"

docker network create kind 2>/dev/null || true

docker ps --format '{{.Names}}' | grep -q '^kindccm$' || {
  docker run --rm -d \
    --name kindccm \
    --network kind \
    --volume /var/run/docker.sock:/var/run/docker.sock \
    "registry.k8s.io/cloud-provider-kind/cloud-controller-manager:v${KINDCCM_VERSION}"
}

#------------------------------------------------------------------------------
# [i] Pull-through image cache
#------------------------------------------------------------------------------

blue "───[ Pull-through registries ]──────────────────────────────────────────"

docker start registry-docker.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
  --restart always \
  --name registry-docker.io \
  --network kind \
  registry:2

docker start registry-quay.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://quay.io \
  --restart always \
  --name registry-quay.io \
  --network kind \
  registry:2

docker start registry-ghcr.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \
  --restart always \
  --name registry-ghcr.io \
  --network kind \
  registry:2

#------------------------------------------------------------------------------
# [i] Create the clusters
#------------------------------------------------------------------------------

blue "───[ KinD clusters ]────────────────────────────────────────────────────"

# Create the temporary directory
[ -d ./tmp ] || mkdir -p ./tmp

# Configure kind
for CELL in $(list cells all "${WLCNT}"); do
  cat <<- EOF > "./tmp/kind-${CELL}.yaml"
		kind: Cluster
		apiVersion: kind.x-k8s.io/v1alpha4
		networking:
		  disableDefaultCNI: true
		  kubeProxyMode: "none"
		kubeadmConfigPatches:
		  - |-
		    kind: ClusterConfiguration
		    networking:
		      dnsDomain: "${CELL}.local"
EOF
done

# Create the clusters
for CELL in $(list cells all "${WLCNT}"); do for CLUSTER in ${CELLS[${CELL}]}; do (
  kind get clusters 2>/dev/null | grep -q "${CLUSTER}" || {
    kind create cluster --name "${CLUSTER}" --config "./tmp/kind-${CELL}.yaml" -q
  }
) & done; done; wait

# Get all the IPs
declare -A IP
for CLUSTER in $(list clusters all "${WLCNT}"); do
  IP[${CLUSTER}]=$(docker inspect "${CLUSTER}-control-plane" |
    jq -r '.[].NetworkSettings.Networks.kind.IPAddress')
  echo "${CLUSTER} = ${IP[${CLUSTER}]}"
done

#------------------------------------------------------------------------------
# [i] Setup KUBECONFIG
#------------------------------------------------------------------------------

blue "───[ KUBECONFIG ]───────────────────────────────────────────────────────"

# Setup an in-cluster reachable API server config
cp -f ~/.kube/config ~/.kube/config.in-cluster
for CLUSTER in $(list clusters all "${WLCNT}"); do
  echo "Server: https://${IP[${CLUSTER}]}:6443"
  kubectl --kubeconfig ~/.kube/config.in-cluster \
  config set-cluster "kind-${CLUSTER}" \
  --server="https://${IP[${CLUSTER}]}:6443"
done

# Helper function k0
function k0 {
  kubectl --context "kind-${MNGR}" "${@}"
}

# Helper function h0
function h0 {
  retry helm --kube-context "kind-${MNGR}" "${@}"
}

#------------------------------------------------------------------------------
# [i] Helm repositories
#------------------------------------------------------------------------------

blue "───[ Helm repos ]───────────────────────────────────────────────────────"

helm repo add cilium https://helm.cilium.io/
helm repo add argo https://argoproj.github.io/argo-helm
helm repo add k8s_gateway https://k8s-gateway.github.io/k8s_gateway/
helm repo update

#------------------------------------------------------------------------------
# [i] Install the Cilium CNI
#------------------------------------------------------------------------------

blue "───[ Cilium CNI ]───────────────────────────────────────────────────────"

# Install the Cilium CNI
for CLUSTER in $(list clusters all "${WLCNT}"); do (

  # Return early if already running
  kubectl --context "kind-${CLUSTER}" -n kube-system \
    get deploy cilium-operator 2>/dev/null && exit 0

  # Install
  retry helm --kube-context "kind-${CLUSTER}" \
    upgrade -i cilium cilium/cilium \
    --version "${CILIUM_CHART_VERSION}" \
    --namespace kube-system \
    --values ./charts/cilium/values.yaml \
    --values ./charts/cilium/values/"${CLUSTER}".yaml \
    --set k8sServiceHost="${IP[${CLUSTER}]}" |
  grep 'LAST DEPLOYED'
) & done; wait

echo

# Wait for all the nodes to be ready
for CLUSTER in $(list clusters all "${WLCNT}"); do (
  kubectl --context "kind-${CLUSTER}" \
    wait --for=condition=Ready nodes \
    --all --timeout=180s
) & done; wait

#------------------------------------------------------------------------------
# [i] Install ClusterMesh
#------------------------------------------------------------------------------

blue "───[ ClusterMesh ]──────────────────────────────────────────────────────"

# With --service-type LoadBalancer, cilium needs connectivity to the external IPs
# Works if it runns on the linux VM, but not on OSX without a route and a TCP proxy.

for CELL in $(list cells wkld "${WLCNT}"); do (

  # Get the clusters of the cell
  IFS=" " read -r -a CLUSTERS <<< "${CELLS[${CELL}]}"

  # Return early if already running
  kubectl --context "kind-${CLUSTERS[0]}" -n kube-system get deploy clustermesh-apiserver 2>/dev/null &&
  kubectl --context "kind-${CLUSTERS[1]}" -n kube-system get deploy clustermesh-apiserver 2>/dev/null &&
  exit 0

  # Shared certificate authority
  kubectl --context "kind-${CLUSTERS[0]}" -n kube-system get secret cilium-ca -o yaml |
  kubectl --context "kind-${CLUSTERS[1]}" -n kube-system replace --force -f -

  # Install
  cilium clustermesh enable --context "kind-${CLUSTERS[0]}" --service-type LoadBalancer
  cilium clustermesh enable --context "kind-${CLUSTERS[1]}" --service-type LoadBalancer
  cilium clustermesh connect --context "kind-${CLUSTERS[0]}" --destination-context "kind-${CLUSTERS[1]}"
) & done; wait

#------------------------------------------------------------------------------
# [i] Install k8s_gateway
#------------------------------------------------------------------------------

blue "───[ k8s_gateway ]──────────────────────────────────────────────────────"

for CLUSTER in $(list clusters all "${WLCNT}"); do (
  retry helm --kube-context "kind-${CLUSTER}" \
    upgrade -i exdns k8s_gateway/k8s-gateway \
    --version "${K8S_GATEWAY_CHART_VERSION}" \
    --namespace kube-system \
    --set "domain=${DOMAIN}" |
  grep 'LAST DEPLOYED'
) & done; wait

#------------------------------------------------------------------------------
# [i] Setup internal CoreDNS for the ${DOMAIN}
#------------------------------------------------------------------------------

blue "───[ CoreDNS ]──────────────────────────────────────────────────────────"

# Get the external LoadBalancer IP
MNGR_EXDNS_IP='null'; until [[ "${MNGR_EXDNS_IP}" != 'null' ]]; do
  MNGR_EXDNS_IP=$(k0 -n kube-system get svc exdns-k8s-gateway -o yaml |
  yq '.status.loadBalancer.ingress[0].ip'); sleep 1
done

# Configure CoreDNS
for CELL in $(list cells all "${WLCNT}"); do for CLUSTER in ${CELLS[${CELL}]}; do (

  kubectl --context "kind-${CLUSTER}" -n kube-system create cm coredns \
  --dry-run=client -o yaml --from-literal=Corefile="
  .:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes ${CELL}.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30 {
       disable success ${CELL}.local
       disable denial ${CELL}.local
    }
    loop
    reload
    loadbalance
  }
  ${DOMAIN}:53 {
    forward . ${MNGR_EXDNS_IP}
  }" | kubectl --context "kind-${CLUSTER}" -n kube-system apply -f - 2>/dev/null

  # Restart otherwise it takes a while to pick up the new config
  kubectl --context "kind-${CLUSTER}" -n kube-system rollout restart deployment coredns

) & done; done; wait

#------------------------------------------------------------------------------
# [i] Setup ArgoCD
#------------------------------------------------------------------------------

blue "───[ ArgoCD ]───────────────────────────────────────────────────────────"

# Install ArgoCD
h0 upgrade --install -n argocd --create-namespace --wait --timeout 5m \
argocd argo/argo-cd --version "${ARGOCD_CHART_VERSION}" \
--set 'server.service.type=LoadBalancer' \
-f - << EOF | grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'
configs:
  cm:
    resource.customizations.ignoreDifferences.admissionregistration.k8s.io_MutatingWebhookConfiguration: |
      jqPathExpressions:
      - '.webhooks[]?.clientConfig.caBundle'
EOF

# Set argocd.${DOMAIN} as ext DNS name
k0 -n argocd annotate svc argocd-server coredns.io/hostname=argocd."${DOMAIN}"

# Get the password
ARGOCD_PASS=$(
  k0 -n argocd \
  get secret argocd-initial-admin-secret \
  -o jsonpath='{.data.password}' | base64 -d
)

# Publish the ArgoCD port
publish "${MNGR}" argocd argocd-server 8080

echo
echo "ArgoCD WebUI: https://$(hostname -I | awk '{print $2}'):8080"
echo "ArgoCD User: admin"
echo "ArgoCD Pass: ${ARGOCD_PASS}"

#------------------------------------------------------------------------------
# [i] Register clusters to ArgoCD
#------------------------------------------------------------------------------

blue "───[ Register clusters to ArgoCD ]──────────────────────────────────────"

# Check if the ArgoCD context exists
argocd context "${MNGR}" 2> /dev/null || {

  # Get the external LoadBalancer IP
  ARGOCD_IP=$(getExtIP "${MNGR}" argocd argocd-server)

  # Login to ArgoCD
  argocd login "${ARGOCD_IP}" \
    --insecure \
    --name "${MNGR}" \
    --username admin \
    --password "${ARGOCD_PASS}"

  # Add clusters with labels
  for CELL in $(list cells wkld "${WLCNT}"); do for CLUSTER in ${CELLS[${CELL}]}; do
    argocd cluster list | grep -q "${CLUSTER}" || \
    argocd cluster add -y "kind-${CLUSTER}" \
      --kubeconfig ~/.kube/config.in-cluster \
      --name "${CLUSTER}" \
      --label name="${CLUSTER}" \
      --label cell="${CELL}"
  done; done
}

#------------------------------------------------------------------------------
# [i] Install all the ApplicationSets
#------------------------------------------------------------------------------

blue "───[ ApplicationSets ]──────────────────────────────────────────────────"

helm template ./charts/cilium --set "chartVersion=${CILIUM_CHART_VERSION}" | k0 apply -f -
helm template ./charts/prometheus --set "chartVersion=${PROMETHEUS_CHART_VERSION}" | k0 apply -f -
helm template ./charts/grafana --set "chartVersion=${GRAFANA_CHART_VERSION}" | k0 apply -f -
helm template ./charts/otel-collector --set "chartVersion=${OTEL_COLLECTOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/vault --set "chartVersion=${VAULT_CHART_VERSION}" | k0 apply -f -
helm template ./charts/cert-manager --set "chartVersion=${CERT_MANAGER_CHART_VERSION}" | k0 apply -f -
helm template ./charts/kubernetes-replicator --set "chartVersion=${KUBERNETES_REPLICATOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/istio --set "chartVersion=${ISTIO_CHART_VERSION}" | k0 apply -f -
helm template ./charts/kiali --set "chartVersion=${KIALI_CHART_VERSION}" | k0 apply -f -

#------------------------------------------------------------------------------
# [i] Setup ArgoWF
#------------------------------------------------------------------------------

blue "───[ ArgoWF ]───────────────────────────────────────────────────────────"

# Install Argo Workflows
h0 upgrade --install -n argowf --create-namespace --wait --timeout 5m \
  argo-workflows argo/argo-workflows --version "${ARGOWF_CHART_VERSION}" \
  --set 'server.serviceType=LoadBalancer' \
  --set 'server.authModes={server}' \
  --set 'server.servicePort=80' \
  --set 'workflow.serviceAccount.create=true' \
  --set 'workflow.serviceAccount.name=argo-workflow' \
  --set 'workflow.rbac.create=true' \
  --set 'controller.workflowNamespaces={argocd}' |
  grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'

# Set argowf.${DOMAIN} as ext DNS name
k0 -n argowf annotate svc argo-workflows-server coredns.io/hostname=argowf."${DOMAIN}"

# Patch the argo-workflow-role to allow patching of the ApplicationSets
k0 -n argocd patch role argo-workflows-workflow --type json \
  -p '[
        {
          "op": "add",
          "path": "/rules/-",
          "value": {
            "apiGroups":["argoproj.io"],
            "resources":["applicationsets"],
            "verbs":["get","watch","patch"]
          }
        }
      ]'

# Publish the ArgoWF port
publish "${MNGR}" argowf argo-workflows-server 8081

echo
echo "ArgoWF WebUI: http://$(hostname -I | awk '{print $2}'):8081"

#------------------------------------------------------------------------------
# [i] Bootstrap DAG
#------------------------------------------------------------------------------

blue "───[ Bootstrap DAG ]────────────────────────────────────────────────────"

# Install Argo WorkflowTemplates
helm template ./charts/wftemplates | k0 -n argocd apply -f -

# Create a secret with ArgoCD credentials
k0 create secret generic argocd-credentials \
 --from-literal=password="${ARGOCD_PASS}" \
 --from-literal=username='admin' \
 --from-literal=token='' \
 --dry-run=client -o yaml | k0 -n argocd apply -f -

# Create the monitoring namespace on all clusters
for CLUSTER in $(list clusters all "${WLCNT}"); do (
  kubectl --context "kind-${CLUSTER}" create ns monitoring \
  --dry-run=client -o yaml | kubectl --context "kind-${CLUSTER}" apply -f -
) & done; wait

# Create the bootstrap workflow
argo --context "kind-${MNGR}" list -n argocd | grep -q bootstrap || \
argo --context "kind-${MNGR}" submit -n argocd -w - << EOF
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: bootstrap-
spec:
  entrypoint: stages
  serviceAccountName: argo-workflow
  templates:
  - name: stages
    dag:
      tasks:
      - name: cilium
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cilium
      - name: prometheus
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=prometheus
      - name: grafana
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=grafana
      - name: otelco-node
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-node
      - name: otelco-cluster
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-cluster
      - name: vault
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=vault
      - name: cert-manager
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cert-manager
      - name: kubernetes-replicator
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=kubernetes-replicator
      - name: populate-vault
        dependencies: [vault]
        templateRef:
          name: populate-vault
          template: populate-vault
      - name: istio-issuers
        dependencies: [populate-vault, cert-manager]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-issuers
      - name: istio-base
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-base
      - name: istio-cni
        dependencies: [istio-base]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-cni
      - name: istio-istiod
        dependencies: [istio-base, istio-issuers]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-istiod
      - name: istio-ztunnel
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ztunnel
      - name: istio-nsgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-nsgw
      - name: istio-ewgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ewgw
      - name: kiali-operator
        dependencies:
        - prometheus
        - grafana
        - istio-cni
        - istio-ztunnel
        - istio-nsgw
        - istio-ewgw
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=kiali-operator
EOF

#------------------------------------------------------------------------------
# [i] Enable Istio endpoint discovery
#------------------------------------------------------------------------------

blue "───[ Istio remote secrets ]─────────────────────────────────────────────"

# Bug? Patch the Istio reader ClusterRole.
# TODO: Remove this if fixed by upstream.
# https://github.com/istio/istio/issues/52739
# https://github.com/istio/istio/issues/42137
for CLUSTER in $(list clusters wkld "${WLCNT}"); do

  # Continue if already patched
  k --context "kind-${CLUSTER}" get clusterrole \
  "istio-reader-clusterrole-${ISTIO_CHART_VERSION//./-}-istio-system" \
  -o jsonpath='{.rules[1].resources}' | grep -q configmaps && continue

  # Patch
  k --context "kind-${CLUSTER}" patch clusterrole \
  "istio-reader-clusterrole-${ISTIO_CHART_VERSION//./-}-istio-system" \
  --type='json' -p='[{"op": "add", "path": "/rules/1/resources/-", "value": "configmaps"}]'
done

for CELL in $(list cells wkld "${WLCNT}"); do (
  IFS=" " read -r -a CLUSTERS <<< "${CELLS[${CELL}]}"
  istioctl --kubeconfig ~/.kube/config.in-cluster create-remote-secret \
    --context "kind-${CLUSTERS[0]}" --name="${CLUSTERS[0]}" | \
    kubectl --context "kind-${CLUSTERS[1]}" apply -f -
  istioctl --kubeconfig ~/.kube/config.in-cluster create-remote-secret \
    --context "kind-${CLUSTERS[1]}" --name="${CLUSTERS[1]}" | \
    kubectl --context "kind-${CLUSTERS[0]}" apply -f -
) & done; wait

#------------------------------------------------------------------------------
# [i] Deploy some workloads
#------------------------------------------------------------------------------

blue "---[ Deploy workloads ]-------------------------------------------------"

swarmctl --context 'pasta-*|pizza-*' i --yes --istio-revision stable --image-tag main
swarmctl --context 'pasta-*|pizza-*' w 1:2 --yes --istio-revision stable --image-tag main

#------------------------------------------------------------------------------
# [i] Publish ports
#------------------------------------------------------------------------------

publish "${MNGR}" vault vault 8082
publish "${MNGR}" monitoring prometheus-server 8083
publish "${MNGR}" monitoring grafana 8084
publish pasta-1 monitoring kiali 8085
