#!/usr/bin/env bash

#------------------------------------------------------------------------------
# [i] Initializations
#------------------------------------------------------------------------------

# Bash strict mode
set -euo pipefail

# Change to the execution directory
cd "$(dirname "$0")"/..

# shellcheck source=../lib/common.sh
source ./lib/common.sh

# One password to rule them all
PWD='meshlab123'

# Runnable sections (in order)
SECTIONS=()

#------------------------------------------------------------------------------
# [i] Versions
#------------------------------------------------------------------------------

KINDCCM_VERSION='0.10.0'                      # https://github.com/kubernetes-sigs/cloud-provider-kind/releases
CILIUM_CHART_VERSION='1.18.5'                 # https://artifacthub.io/packages/helm/cilium/cilium
K8S_GATEWAY_CHART_VERSION='3.2.8'             # https://github.com/k8s-gateway/k8s_gateway/blob/master/charts/k8s-gateway/Chart.yaml
ARGOCD_CHART_VERSION='9.1.9'                  # https://artifacthub.io/packages/helm/argo-cd-oci/argo-cd
ARGOWF_CHART_VERSION='0.46.2'                 # https://artifacthub.io/packages/helm/argo/argo-workflows
PROMETHEUS_CHART_VERSION='27.52.0'            # https://artifacthub.io/packages/helm/prometheus-community/prometheus
GRAFANA_CHART_VERSION='10.4.0'                # https://artifacthub.io/packages/helm/grafana/grafana
OTEL_COLLECTOR_CHART_VERSION='0.142.0'        # https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector
VAULT_CHART_VERSION='0.31.0'                  # https://artifacthub.io/packages/helm/hashicorp/vault
CERT_MANAGER_CHART_VERSION='v1.19.2'          # https://artifacthub.io/packages/helm/cert-manager/cert-manager
KUBERNETES_REPLICATOR_CHART_VERSION='2.12.2'  # https://artifacthub.io/packages/helm/kubernetes-replicator/kubernetes-replicator
ISTIO_CHART_VERSION='1.28.2'                  # https://artifacthub.io/packages/helm/istio-official/base
KIALI_CHART_VERSION='2.20.0'                  # https://artifacthub.io/packages/helm/kiali/kiali-operator

#------------------------------------------------------------------------------
# [i] cloud-provider-kind
#------------------------------------------------------------------------------

cloud-provider-kind() {
  blue "───[ cloud-provider-kind ]──────────────────────────────────────────────"

  docker network create kind 2>/dev/null || true

  docker ps --format '{{.Names}}' | grep -q '^kindccm$' || {
    docker run --rm -d \
      --name kindccm \
      --network kind \
      --volume /var/run/docker.sock:/var/run/docker.sock \
      "registry.k8s.io/cloud-provider-kind/cloud-controller-manager:v${KINDCCM_VERSION}"
  }
}; SECTIONS+=( cloud-provider-kind )

#------------------------------------------------------------------------------
# [i] Pull-through image cache
#------------------------------------------------------------------------------

pull-through-cache() {
  blue "───[ Pull-through registries ]──────────────────────────────────────────"

  docker start registry-docker.io 2>/dev/null || docker run -d \
    -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
    --restart always \
    --name registry-docker.io \
    --network kind \
    registry:2

  docker start registry-quay.io 2>/dev/null || docker run -d \
    -e REGISTRY_PROXY_REMOTEURL=https://quay.io \
    --restart always \
    --name registry-quay.io \
    --network kind \
    registry:2

  docker start registry-ghcr.io 2>/dev/null || docker run -d \
    -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \
    --restart always \
    --name registry-ghcr.io \
    --network kind \
    registry:2
}; SECTIONS+=( pull-through-cache )

#------------------------------------------------------------------------------
# [i] Create the clusters
#------------------------------------------------------------------------------

create-clusters() {
  blue "───[ KinD clusters ]────────────────────────────────────────────────────"

  # Create the temporary directory
  [ -d ./tmp ] || mkdir -p ./tmp

  # Configure kind
  for CELL in $(list cells all "${WLCNT}"); do
    cat <<- EOF > "./tmp/kind-${CELL}.yaml"
		kind: Cluster
		apiVersion: kind.x-k8s.io/v1alpha4
		networking:
		  disableDefaultCNI: true
		  kubeProxyMode: "none"
		kubeadmConfigPatches:
		  - |-
		    kind: ClusterConfiguration
		    networking:
		      dnsDomain: "${CELL}.local"
		EOF
  done

  # Create the clusters
  for CELL in $(list cells all "${WLCNT}"); do for CLUSTER in ${CELLS[${CELL}]}; do (
    kind get clusters 2>/dev/null | grep -q "${CLUSTER}" || {
      kind create cluster --name "${CLUSTER}" --config "./tmp/kind-${CELL}.yaml" -q
    }
  ) & done; done; wait

  # Get and display all the IPs
  ensure-ips
  for CLUSTER in $(list clusters all "${WLCNT}"); do
    echo "${CLUSTER} = ${IP[${CLUSTER}]}"
  done
}; SECTIONS+=( create-clusters )

#------------------------------------------------------------------------------
# [i] Add the registries to containerd
#------------------------------------------------------------------------------

add-registries-to-containerd() {
  blue "───[ Configure containerd ]─────────────────────────────────────────────"

  # Map registries to pull-through caches
  echo "docker.io --> http://registry-docker.io:5000"
  echo "quay.io   --> http://registry-quay.io:5000"
  echo "ghcr.io   --> http://registry-ghcr.io:5000"

  # Configure containerd in all clusters
  for CLUSTER in $(list clusters all "${WLCNT}"); do (
    docker exec -i "${CLUSTER}-control-plane" bash -c '
      mkdir -p /etc/containerd/certs.d/docker.io
      cat <<- EOF > /etc/containerd/certs.d/docker.io/hosts.toml
			server = "https://registry-1.docker.io"
			[host."http://registry-docker.io:5000"]
			  capabilities = ["pull", "resolve"]
			EOF
      mkdir -p /etc/containerd/certs.d/quay.io
      cat <<- EOF > /etc/containerd/certs.d/quay.io/hosts.toml
			server = "https://quay.io"
			[host."http://registry-quay.io:5000"]
			  capabilities = ["pull", "resolve"]
			EOF
      mkdir -p /etc/containerd/certs.d/ghcr.io
      cat <<- EOF > /etc/containerd/certs.d/ghcr.io/hosts.toml
			server = "https://ghcr.io"
			[host."http://registry-ghcr.io:5000"]
			  capabilities = ["pull", "resolve"]
			EOF
    '
  ) & done; wait
}; SECTIONS+=( add-registries-to-containerd )

#------------------------------------------------------------------------------
# [i] Setup KUBECONFIG
#------------------------------------------------------------------------------

setup-kubeconfig() {
  blue "───[ KUBECONFIG ]───────────────────────────────────────────────────────"
  ensure-ips # Ensure cluster IPs are initialized

  # Setup an in-cluster reachable API server config
  cp -f ~/.kube/config ~/.kube/config.in-cluster
  for CLUSTER in $(list clusters all "${WLCNT}"); do
    echo "Server: https://${IP[${CLUSTER}]}:6443"
    kubectl --kubeconfig ~/.kube/config.in-cluster \
    config set-cluster "kind-${CLUSTER}" \
    --server="https://${IP[${CLUSTER}]}:6443"
  done
}; SECTIONS+=( setup-kubeconfig )

#------------------------------------------------------------------------------
# [i] Helm repositories
#------------------------------------------------------------------------------

helm-repositories() {
  blue "───[ Helm repos ]───────────────────────────────────────────────────────"

  helm repo add cilium https://helm.cilium.io/
  helm repo add argo https://argoproj.github.io/argo-helm
  helm repo add k8s_gateway https://k8s-gateway.github.io/k8s_gateway/
  helm repo update
}; SECTIONS+=( helm-repositories )

#------------------------------------------------------------------------------
# [i] Install the Cilium CNI
#------------------------------------------------------------------------------

install-cilium() {
  blue "───[ Cilium CNI ]───────────────────────────────────────────────────────"

  # Install the Cilium CNI
  for CLUSTER in $(list clusters all "${WLCNT}"); do (

    # Return early if already running
    kubectl --context "kind-${CLUSTER}" -n kube-system \
      get deploy cilium-operator 2>/dev/null && exit 0

    # Install
    retry helm --kube-context "kind-${CLUSTER}" \
      upgrade -i cilium cilium/cilium \
      --version "${CILIUM_CHART_VERSION}" \
      --namespace kube-system \
      --values ./charts/cilium/values.yaml \
      --values ./charts/cilium/values/"${CLUSTER}".yaml \
      --set k8sServiceHost="${IP[${CLUSTER}]}" |
    grep 'LAST DEPLOYED'
  ) & done; wait

  echo

  # Wait for all the nodes to be ready
  for CLUSTER in $(list clusters all "${WLCNT}"); do (
    kubectl --context "kind-${CLUSTER}" \
      wait --for=condition=Ready nodes \
      --all --timeout=180s
  ) & done; wait
}; SECTIONS+=( install-cilium )

#------------------------------------------------------------------------------
# [i] Install ClusterMesh
#------------------------------------------------------------------------------

install-clustermesh() {
  blue "───[ ClusterMesh ]──────────────────────────────────────────────────────"

  # With --service-type LoadBalancer, cilium needs connectivity to the external IPs
  # Works if it runns on the linux VM, but not on OSX without a route and a TCP proxy.

  for CELL in $(list cells wkld "${WLCNT}"); do (

    # Get the clusters of the cell
    IFS=" " read -r -a CLUSTERS <<< "${CELLS[${CELL}]}"

    # Return early if already running
    kubectl --context "kind-${CLUSTERS[0]}" -n kube-system get deploy clustermesh-apiserver 2>/dev/null &&
    kubectl --context "kind-${CLUSTERS[1]}" -n kube-system get deploy clustermesh-apiserver 2>/dev/null &&
    exit 0

    # Shared certificate authority
    kubectl --context "kind-${CLUSTERS[0]}" -n kube-system get secret cilium-ca -o yaml |
    kubectl --context "kind-${CLUSTERS[1]}" -n kube-system replace --force -f -

    # Install
    cilium clustermesh enable --context "kind-${CLUSTERS[0]}" --service-type LoadBalancer
    cilium clustermesh enable --context "kind-${CLUSTERS[1]}" --service-type LoadBalancer
    cilium clustermesh connect --context "kind-${CLUSTERS[0]}" --destination-context "kind-${CLUSTERS[1]}"
  ) & done; wait
}; SECTIONS+=( install-clustermesh )

#------------------------------------------------------------------------------
# [i] Install k8s_gateway
#------------------------------------------------------------------------------

install-k8s-gateway() {
  blue "───[ k8s_gateway ]──────────────────────────────────────────────────────"

  for CLUSTER in $(list clusters all "${WLCNT}"); do (
    retry helm --kube-context "kind-${CLUSTER}" \
      upgrade -i exdns k8s_gateway/k8s-gateway \
      --version "${K8S_GATEWAY_CHART_VERSION}" \
      --namespace kube-system \
      --set "domain=${DOMAIN}" |
    grep 'LAST DEPLOYED'
  ) & done; wait
}; SECTIONS+=( install-k8s-gateway )

#------------------------------------------------------------------------------
# [i] Validate the command line arguments
#------------------------------------------------------------------------------

case "${1:-}" in
  create|delete) WLCNT="${2:-1}" ;;
  run)  WLCNT="${3:-1}"; "${2:?Usage: $0 run <section> [count]}"; exit 0 ;;
  list) printf '%s\n' "${SECTIONS[@]}"; exit 0 ;;
  *)    echo "Usage: $(basename "$0") <create|delete|list|run SECTION [count]>"; exit 1 ;;
esac

#------------------------------------------------------------------------------
# [i] Handle the delete command
#------------------------------------------------------------------------------

if [[ $1 == "delete" ]]; then

  blue "───[ Cleanup ]────────────────────────────────────────────────────────"

  # shellcheck disable=SC2046
  docker kill $(docker ps -f "name=kindccm" -q) 2>/dev/null || true
  argocd context --delete "${MNGR}" 2>/dev/null || true
  kind delete clusters --all
  rm -rf ./tmp/*
  exit 0
fi

#------------------------------------------------------------------------------
# All sections in order
#------------------------------------------------------------------------------

for section in "${SECTIONS[@]}"; do "$section"; done

#------------------------------------------------------------------------------
# [i] Setup internal CoreDNS for the ${DOMAIN}
#------------------------------------------------------------------------------

blue "───[ CoreDNS ]──────────────────────────────────────────────────────────"

# Get the external LoadBalancer IP
MNGR_EXDNS_IP='null'; until [[ "${MNGR_EXDNS_IP}" != 'null' ]]; do
  MNGR_EXDNS_IP=$(k0 -n kube-system get svc exdns-k8s-gateway -o yaml |
  yq '.status.loadBalancer.ingress[0].ip'); sleep 1
done

# Configure CoreDNS
for CELL in $(list cells all "${WLCNT}"); do for CLUSTER in ${CELLS[${CELL}]}; do (

  kubectl --context "kind-${CLUSTER}" -n kube-system create cm coredns \
  --dry-run=client -o yaml --from-literal=Corefile="
  .:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes ${CELL}.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30 {
       disable success ${CELL}.local
       disable denial ${CELL}.local
    }
    loop
    reload
    loadbalance
  }
  ${DOMAIN}:53 {
    forward . ${MNGR_EXDNS_IP}
  }" | kubectl --context "kind-${CLUSTER}" -n kube-system apply -f - 2>/dev/null

  # Restart otherwise it takes a while to pick up the new config
  kubectl --context "kind-${CLUSTER}" -n kube-system rollout restart deployment coredns

) & done; done; wait

#------------------------------------------------------------------------------
# [i] Setup ArgoCD
#------------------------------------------------------------------------------

blue "───[ ArgoCD ]───────────────────────────────────────────────────────────"

# Install ArgoCD
h0 upgrade --install -n argocd --create-namespace --wait --timeout 5m \
argocd argo/argo-cd --version "${ARGOCD_CHART_VERSION}" \
--set 'server.service.type=LoadBalancer' \
--set 'server.extraArgs={--insecure=true}' \
--set configs.secret.argocdServerAdminPassword=$(htpasswd -nbBC 10 "" ${PWD} | tr -d ':\n') \
-f - << EOF | grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'
configs:
  cm:
    resource.customizations.ignoreDifferences.admissionregistration.k8s.io_MutatingWebhookConfiguration: |
      jqPathExpressions:
      - '.webhooks[]?.clientConfig.caBundle'
EOF

# Set argocd.${DOMAIN} as ext DNS name
k0 -n argocd annotate svc argocd-server coredns.io/hostname=argocd."${DOMAIN}"

# Publish the ArgoCD port
publish "${MNGR}" argocd argocd-server 8080

#------------------------------------------------------------------------------
# [i] Register clusters to ArgoCD
#------------------------------------------------------------------------------

blue "───[ Register clusters to ArgoCD ]──────────────────────────────────────"

# Check if the ArgoCD context exists
argocd context "${MNGR}" 2> /dev/null || {

  # Get the external LoadBalancer IP
  ARGOCD_IP=$(getExtIP "${MNGR}" argocd argocd-server)

  # Login to ArgoCD
  argocd login "${ARGOCD_IP}" \
    --plaintext \
    --name "${MNGR}" \
    --username admin \
    --password "${PWD}"

  # Add clusters with labels
  for CELL in $(list cells wkld "${WLCNT}"); do for CLUSTER in ${CELLS[${CELL}]}; do
    argocd cluster list | grep -q "${CLUSTER}" || \
    argocd cluster add -y "kind-${CLUSTER}" \
      --kubeconfig ~/.kube/config.in-cluster \
      --name "${CLUSTER}" \
      --label name="${CLUSTER}" \
      --label cell="${CELL}"
  done; done
}

#------------------------------------------------------------------------------
# [i] Install all the ApplicationSets
#------------------------------------------------------------------------------

blue "───[ ApplicationSets ]──────────────────────────────────────────────────"

helm template ./charts/cilium --set "chartVersion=${CILIUM_CHART_VERSION}" | k0 apply -f -
helm template ./charts/prometheus --set "chartVersion=${PROMETHEUS_CHART_VERSION}" | k0 apply -f -
helm template ./charts/grafana --set "chartVersion=${GRAFANA_CHART_VERSION}" | k0 apply -f -
helm template ./charts/otel-collector --set "chartVersion=${OTEL_COLLECTOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/vault --set "chartVersion=${VAULT_CHART_VERSION}" | k0 apply -f -
helm template ./charts/cert-manager --set "chartVersion=${CERT_MANAGER_CHART_VERSION}" | k0 apply -f -
helm template ./charts/kubernetes-replicator --set "chartVersion=${KUBERNETES_REPLICATOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/istio --set "chartVersion=${ISTIO_CHART_VERSION}" | k0 apply -f -
helm template ./charts/kiali --set "chartVersion=${KIALI_CHART_VERSION}" | k0 apply -f -

#------------------------------------------------------------------------------
# [i] Setup ArgoWF
#------------------------------------------------------------------------------

blue "───[ ArgoWF ]───────────────────────────────────────────────────────────"

# Install Argo Workflows
h0 upgrade --install -n argowf --create-namespace --wait --timeout 5m \
  argo-workflows argo/argo-workflows --version "${ARGOWF_CHART_VERSION}" \
  --set 'server.serviceType=LoadBalancer' \
  --set 'server.authModes={server}' \
  --set 'server.servicePort=80' \
  --set 'workflow.serviceAccount.create=true' \
  --set 'workflow.serviceAccount.name=argo-workflow' \
  --set 'workflow.rbac.create=true' \
  --set 'controller.workflowNamespaces={argocd}' \
  --set-json 'workflow.rbac.rules=[{"apiGroups":["argoproj.io"],"resources":["applicationsets"],"verbs":["get","watch","patch"]}]' |
  grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'

# Set argowf.${DOMAIN} as ext DNS name
k0 -n argowf annotate svc argo-workflows-server coredns.io/hostname=argowf."${DOMAIN}"

# Publish the ArgoWF port
publish "${MNGR}" argowf argo-workflows-server 8081

#------------------------------------------------------------------------------
# [i] Bootstrap DAG
#------------------------------------------------------------------------------

blue "───[ Bootstrap DAG ]────────────────────────────────────────────────────"

# Install Argo WorkflowTemplates
helm template ./charts/wftemplates | k0 -n argocd apply -f -

# Create a secret with ArgoCD credentials
k0 create secret generic argocd-credentials \
 --from-literal=username='admin' \
 --from-literal=password="${PWD}" \
 --from-literal=token='' \
 --dry-run=client -o yaml | k0 -n argocd apply -f -

# Create the monitoring namespace on all clusters
for CLUSTER in $(list clusters all "${WLCNT}"); do (
  kubectl --context "kind-${CLUSTER}" create ns monitoring \
  --dry-run=client -o yaml | kubectl --context "kind-${CLUSTER}" apply -f -
) & done; wait

# Create the bootstrap workflow
argo --context "kind-${MNGR}" list -n argocd | grep -q bootstrap || \
argo --context "kind-${MNGR}" submit -n argocd -w - << EOF
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: bootstrap-
spec:
  entrypoint: stages
  serviceAccountName: argo-workflow
  templates:
  - name: stages
    dag:
      tasks:
      - name: cilium
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cilium
      - name: prometheus
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=prometheus
      - name: grafana
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=grafana
      - name: otelco-node
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-node
      - name: otelco-cluster
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-cluster
      - name: vault
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=vault
      - name: cert-manager
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cert-manager
      - name: kubernetes-replicator
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=kubernetes-replicator
      - name: populate-vault
        dependencies: [vault]
        templateRef:
          name: populate-vault
          template: populate-vault
      - name: istio-issuers
        dependencies: [populate-vault, cert-manager]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-issuers
      - name: istio-base
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-base
      - name: istio-cni
        dependencies: [istio-base]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-cni
      - name: istio-istiod
        dependencies: [istio-base, istio-issuers]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-istiod
      - name: istio-ztunnel
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ztunnel
      - name: istio-nsgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-nsgw
      - name: istio-ewgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ewgw
      - name: kiali-operator
        dependencies:
        - prometheus
        - grafana
        - istio-cni
        - istio-ztunnel
        - istio-nsgw
        - istio-ewgw
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=kiali-operator
EOF

#------------------------------------------------------------------------------
# [i] Enable Istio endpoint discovery
#------------------------------------------------------------------------------

blue "───[ Istio remote secrets ]─────────────────────────────────────────────"

# Bug? Patch the Istio reader ClusterRole.
# TODO: Remove this if fixed by upstream.
# https://github.com/istio/istio/issues/52739
# https://github.com/istio/istio/issues/42137
for CLUSTER in $(list clusters wkld "${WLCNT}"); do

  # Continue if already patched
  k --context "kind-${CLUSTER}" get clusterrole \
  "istio-reader-clusterrole-${ISTIO_CHART_VERSION//./-}-istio-system" \
  -o jsonpath='{.rules[1].resources}' | grep -q configmaps && continue

  # Patch
  k --context "kind-${CLUSTER}" patch clusterrole \
  "istio-reader-clusterrole-${ISTIO_CHART_VERSION//./-}-istio-system" \
  --type='json' -p='[{"op": "add", "path": "/rules/1/resources/-", "value": "configmaps"}]'
done

for CELL in $(list cells wkld "${WLCNT}"); do (
  IFS=" " read -r -a CLUSTERS <<< "${CELLS[${CELL}]}"
  istioctl --kubeconfig ~/.kube/config.in-cluster create-remote-secret \
    --context "kind-${CLUSTERS[0]}" --name="${CLUSTERS[0]}" | \
    kubectl --context "kind-${CLUSTERS[1]}" apply -f -
  istioctl --kubeconfig ~/.kube/config.in-cluster create-remote-secret \
    --context "kind-${CLUSTERS[1]}" --name="${CLUSTERS[1]}" | \
    kubectl --context "kind-${CLUSTERS[0]}" apply -f -
) & done; wait

#------------------------------------------------------------------------------
# [i] External kiali multi-cluster setup
#------------------------------------------------------------------------------

blue "───[ External kiali multi-cluster setup ]───────────────────────────────────────"

for CLUSTER in $(list clusters wkld "${WLCNT}"); do
  kiali-prepare-remote-cluster.sh \
    --kiali-cluster-context "kind-${MNGR}" \
    --remote-cluster-context "kind-${CLUSTER}" \
    --view-only false \
    --process-kiali-secret true \
    --process-remote-resources true \
    --kiali-cluster-namespace monitoring \
    --remote-cluster-name "${CLUSTER}" \
    --remote-cluster-url "https://${IP[${CLUSTER}]}:6443" |
    grep -v '^INFO:'
done

k0 -n monitoring annotate kiali kiali kiali.io/reconcile="$(date +%s)" --overwrite

#------------------------------------------------------------------------------
# [i] Kiali-Grafana integration
#------------------------------------------------------------------------------

blue "───[ Kiali-Grafana integration ]────────────────────────────────────────"

GRAFANA_URL="http://$(getExtIP "${MNGR}" monitoring grafana)"
GRAFANA_AUTH="admin:${PWD}"

# Create service account
SA_ID=$(curl -sS -u "${GRAFANA_AUTH}" -H 'Content-Type: application/json' \
  -d '{"name":"kiali","role":"Viewer"}' "${GRAFANA_URL}/api/serviceaccounts" | jq '.id')

# Create token and patch Kiali
[[ "${SA_ID}" =~ ^[0-9]+$ ]] && \
TOKEN=$(curl -sS -u "${GRAFANA_AUTH}" -H 'Content-Type: application/json' \
  -d '{"name":"kiali-token"}' "${GRAFANA_URL}/api/serviceaccounts/${SA_ID}/tokens" | jq -r '.key') && \
kubectl --context "kind-${MNGR}" -n monitoring patch kiali kiali --type=merge \
  -p '{"spec":{"external_services":{"grafana":{"auth":{"type":"token","token":"'"${TOKEN}"'","use_kiali_token":false}}}}}'

#------------------------------------------------------------------------------
# [i] Deploy some workloads
#------------------------------------------------------------------------------

blue "---[ Deploy workloads ]-------------------------------------------------"

swarmctl --context 'pasta-*|pizza-*' i --yes --istio-revision stable --image-tag main
swarmctl --context 'pasta-*|pizza-*' w 1:2 --yes --istio-revision stable --image-tag main

#------------------------------------------------------------------------------
# [i] Publish ports
#------------------------------------------------------------------------------

publish "${MNGR}" vault vault 8082
publish "${MNGR}" monitoring prometheus-server 8083
publish "${MNGR}" monitoring grafana 8084
publish "${MNGR}" monitoring kiali 8085
