#!/usr/bin/env bash

#------------------------------------------------------------------------------
# [i] Initializations
#------------------------------------------------------------------------------

# Bash strict mode
set -euo pipefail

# Change to the execution directory
cd "$(dirname "$0")"/..

# shellcheck source=../lib/common.sh
source ./lib/common.sh

#------------------------------------------------------------------------------
# [i] Versions
#------------------------------------------------------------------------------

CILIUM_CHART_VERSION='1.16.5'                 # https://artifacthub.io/packages/helm/cilium/cilium
K8S_GATEWAY_CHART_VERSION='2.4.0'             # https://github.com/ori-edge/k8s_gateway/tree/master/charts
ARGOCD_CHART_VERSION='7.7.12'                 # https://artifacthub.io/packages/helm/argo-cd-oci/argo-cd
ARGOWF_CHART_VERSION='0.45.2'                 # https://artifacthub.io/packages/helm/argo/argo-workflows
PROMETHEUS_CHART_VERSION='26.0.1'             # https://artifacthub.io/packages/helm/prometheus-community/prometheus
GRAFANA_CHART_VERSION='8.8.2'                 # https://artifacthub.io/packages/helm/grafana/grafana
OTEL_COLLECTOR_CHART_VERSION='0.111.0'        # https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector
VAULT_CHART_VERSION='0.29.1'                  # https://artifacthub.io/packages/helm/hashicorp/vault
CERT_MANAGER_CHART_VERSION='v1.16.2'          # https://artifacthub.io/packages/helm/cert-manager/cert-manager
KUBERNETES_REPLICATOR_CHART_VERSION='2.11.0'  # https://artifacthub.io/packages/helm/kubernetes-replicator/kubernetes-replicator
ISTIO_CHART_VERSION='1.24.2'                  # https://artifacthub.io/packages/helm/istio-official/base

#------------------------------------------------------------------------------
# [i] Validate the command line arguments
#------------------------------------------------------------------------------

[[ $# -ne 1 || ($1 != "create" && $1 != "delete") ]] && \
{ echo "Usage: $0 <create|delete>"; exit 1; }

#------------------------------------------------------------------------------
# [i] Handle the delete command
#------------------------------------------------------------------------------

if [[ $1 == "delete" ]]; then

  blue "───[ Cleanup ]────────────────────────────────────────────────────────"

  # Cleanup the iptables chains
  iptables -t nat -D PREROUTING -j MESHLABPRE 2>/dev/null || true
  iptables -t nat -D POSTROUTING -j MESHLABPOS 2>/dev/null || true
  iptables -t nat -F MESHLABPRE 2>/dev/null || true
  iptables -t nat -F MESHLABPOS 2>/dev/null || true
  iptables -t nat -X MESHLABPRE 2>/dev/null || true
  iptables -t nat -X MESHLABPOS 2>/dev/null || true

  # shellcheck disable=SC2046
  docker kill $(docker ps -f "name=kindccm" -q) 2>/dev/null || true
  argocd context --delete "${MNGR}" 2>/dev/null || true
  kind delete clusters --all
  rm -rf ./tmp/*
  exit 0
fi

#------------------------------------------------------------------------------
# [i] Initialize the MeshLab iptables chains
#------------------------------------------------------------------------------

blue "───[ Iptables chains ]──────────────────────────────────────────────────"

iptables -t nat -L MESHLABPRE &>/dev/null || iptables -t nat -N MESHLABPRE
iptables -t nat -C PREROUTING -j MESHLABPRE &>/dev/null || iptables -t nat -A PREROUTING -j MESHLABPRE

iptables -t nat -L MESHLABPOS &>/dev/null || iptables -t nat -N MESHLABPOS
iptables -t nat -C POSTROUTING -j MESHLABPOS &>/dev/null || iptables -t nat -A POSTROUTING -j MESHLABPOS

iptables -t nat -nL MESHLABPRE
iptables -t nat -nL MESHLABPOS

#------------------------------------------------------------------------------
# [i] cloud-provider-kind
#------------------------------------------------------------------------------

blue "───[ cloud-provider-kind ]──────────────────────────────────────────────"

docker network create kind 2>/dev/null || true

docker ps --format '{{.Names}}' | grep -q '^kindccm$' || {
  docker run --rm -d \
    --name kindccm \
    --network kind \
    --volume /var/run/docker.sock:/var/run/docker.sock \
    registry.k8s.io/cloud-provider-kind/cloud-controller-manager:v0.4.0
}

#------------------------------------------------------------------------------
# [i] Pull-through image cache
#------------------------------------------------------------------------------

blue "───[ Pull-through registries ]──────────────────────────────────────────"

docker start registry-docker.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
  --restart always \
  --name registry-docker.io \
  --network kind \
  registry:2

docker start registry-quay.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://quay.io \
  --restart always \
  --name registry-quay.io \
  --network kind \
  registry:2

docker start registry-ghcr.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \
  --restart always \
  --name registry-ghcr.io \
  --network kind \
  registry:2

#------------------------------------------------------------------------------
# [i] Create the clusters
#------------------------------------------------------------------------------

blue "───[ KinD clusters ]────────────────────────────────────────────────────"

# Create the temporary directory
[ -d ./tmp ] || mkdir -p ./tmp

# Configure kind
for CELL in $(list cells); do
  cat <<- EOF > "./tmp/kind-${CELL}.yaml"
		kind: Cluster
		apiVersion: kind.x-k8s.io/v1alpha4
		networking:
		  disableDefaultCNI: true
		  kubeProxyMode: "none"
		containerdConfigPatches:
		  - |-
		    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
		      endpoint = ["http://registry-docker.io:5000"]
		    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]
		      endpoint = ["http://registry-quay.io:5000"]
		    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."ghcr.io"]
		      endpoint = ["http://registry-ghcr.io:5000"]
		kubeadmConfigPatches:
		  - |-
		    kind: ClusterConfiguration
		    networking:
		      dnsDomain: "${CELL}.local"
EOF
done

# Create the clusters
for CELL in $(list cells); do for CLUSTER in ${CELLS[${CELL}]}; do (
  kind get clusters 2>/dev/null | grep -q "${CLUSTER}" || {
    kind create cluster --name "${CLUSTER}" --config "./tmp/kind-${CELL}.yaml" -q
  }
) & done; done; wait

# Get all the IPs
declare -A IP
for CLUSTER in $(list clusters); do
  IP[${CLUSTER}]=$(docker inspect "${CLUSTER}-control-plane" |
    jq -r '.[].NetworkSettings.Networks.kind.IPAddress')
  echo "${CLUSTER} = ${IP[${CLUSTER}]}"
done

#------------------------------------------------------------------------------
# [i] Setup KUBECONFIG
#------------------------------------------------------------------------------

blue "───[ KUBECONFIG ]───────────────────────────────────────────────────────"

# Use routable IPs
for CLUSTER in $(list clusters); do
  SERVER="https://${IP[${CLUSTER}]}:6443"; echo "Server: ${SERVER}"
  kubectl config set-cluster "kind-${CLUSTER}" --server="${SERVER}"
done

function k0 {
  kubectl --context "kind-${MNGR}" "${@}"
}

function h0 {
  helm --kube-context "kind-${MNGR}" "${@}"
}

#------------------------------------------------------------------------------
# [i] Helm repositories
#------------------------------------------------------------------------------

blue "───[ Helm repos ]───────────────────────────────────────────────────────"

helm repo add cilium https://helm.cilium.io/
helm repo add argo https://argoproj.github.io/argo-helm
helm repo add k8s_gateway https://ori-edge.github.io/k8s_gateway
helm repo update

#------------------------------------------------------------------------------
# [i] Install the Cilium CNI
#------------------------------------------------------------------------------

blue "───[ Cilium CNI ]───────────────────────────────────────────────────────"

# Install the Cilium CNI
for CLUSTER in $(list clusters); do (

  # Return early if already running
  kubectl --context "kind-${CLUSTER}" -n kube-system \
    get deploy cilium-operator 2>/dev/null && exit 0

  # Install
  helm --kube-context "kind-${CLUSTER}" \
    upgrade -i cilium cilium/cilium \
    --version "${CILIUM_CHART_VERSION}" \
    --namespace kube-system \
    --values ./charts/cilium/values.yaml \
    --values ./charts/cilium/values/"${CLUSTER}".yaml \
    --set k8sServiceHost="${IP[${CLUSTER}]}" |
  grep 'LAST DEPLOYED'
) & done; wait

echo

# Wait for all the nodes to be ready
for CLUSTER in $(list clusters); do (
  kubectl --context "kind-${CLUSTER}" \
    wait --for=condition=Ready nodes \
    --all --timeout=120s
) & done; wait

#------------------------------------------------------------------------------
# [i] Install ClusterMesh
#------------------------------------------------------------------------------

blue "───[ ClusterMesh ]──────────────────────────────────────────────────────"

# With --service-type LoadBalancer, cilium needs connectivity to the external IPs
# Works if it runns on the linux VM, but not on OSX without a route and a TCP proxy.

for CELL in $(list cells wkld); do (

  # Get the clusters of the cell
  IFS=" " read -r -a CLUSTERS <<< "${CELLS[${CELL}]}"

  # Return early if already running
  kubectl --context "kind-${CLUSTERS[0]}" -n kube-system get deploy clustermesh-apiserver 2>/dev/null &&
  kubectl --context "kind-${CLUSTERS[1]}" -n kube-system get deploy clustermesh-apiserver 2>/dev/null &&
  exit 0

  # Install
  cilium clustermesh enable --context "kind-${CLUSTERS[0]}" --service-type LoadBalancer
  cilium clustermesh enable --context "kind-${CLUSTERS[1]}" --service-type LoadBalancer
  cilium clustermesh connect --context "kind-${CLUSTERS[0]}" --destination-context "kind-${CLUSTERS[1]}"
) & done; wait

#------------------------------------------------------------------------------
# [i] Install k8s_gateway
#------------------------------------------------------------------------------

blue "───[ k8s_gateway ]──────────────────────────────────────────────────────"

for CLUSTER in $(list clusters); do (
  helm --kube-context "kind-${CLUSTER}" \
    upgrade -i exdns k8s_gateway/k8s-gateway \
    --version "${K8S_GATEWAY_CHART_VERSION}" \
    --namespace kube-system \
    --set "domain=${DOMAIN}" |
  grep 'LAST DEPLOYED'
) & done; wait

#------------------------------------------------------------------------------
# [i] Setup internal CoreDNS for the ${DOMAIN}
#------------------------------------------------------------------------------

blue "───[ CoreDNS ]──────────────────────────────────────────────────────────"

# Get the external LoadBalancer IP
MNGR_EXDNS_IP='null'; until [[ "${MNGR_EXDNS_IP}" != 'null' ]]; do
  MNGR_EXDNS_IP=$(k0 -n kube-system get svc exdns-k8s-gateway -o yaml |
  yq '.status.loadBalancer.ingress[0].ip'); sleep 1
done

# Configure CoreDNS
for CELL in $(list cells); do for CLUSTER in ${CELLS[${CELL}]}; do (

  kubectl --context "kind-${CLUSTER}" -n kube-system create cm coredns \
  --dry-run=client -o yaml --from-literal=Corefile="
  .:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes ${CELL}.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30 {
       disable success ${CELL}.local
       disable denial ${CELL}.local
    }
    loop
    reload
    loadbalance
  }
  ${DOMAIN}:53 {
    forward . ${MNGR_EXDNS_IP}
  }" | kubectl --context "kind-${CLUSTER}" -n kube-system apply -f - 2>/dev/null

  # Restart otherwise it takes a while to pick up the new config
  kubectl --context "kind-${CLUSTER}" -n kube-system rollout restart deployment coredns

) & done; done; wait

#------------------------------------------------------------------------------
# [i] Setup ArgoCD
#------------------------------------------------------------------------------

blue "───[ ArgoCD ]───────────────────────────────────────────────────────────"

# Install ArgoCD
h0 upgrade --install -n argocd --create-namespace --wait --timeout 5m \
argocd argo/argo-cd --version "${ARGOCD_CHART_VERSION}" \
--set 'server.service.type=LoadBalancer' \
-f - << EOF | grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'
configs:
  cm:
    resource.customizations.ignoreDifferences.admissionregistration.k8s.io_MutatingWebhookConfiguration: |
      jqPathExpressions:
      - '.webhooks[]?.clientConfig.caBundle'
EOF

# Set argocd.${DOMAIN} as ext DNS name
k0 -n argocd annotate svc argocd-server coredns.io/hostname=argocd."${DOMAIN}"

# Get the password
ARGOCD_PASS=$(
  k0 -n argocd \
  get secret argocd-initial-admin-secret \
  -o jsonpath='{.data.password}' | base64 -d
)

# Publish the ArgoCD port
publish "${MNGR}" argocd argocd-server 8080

echo
echo "ArgoCD WebUI: https://$(hostname -I | awk '{print $2}'):8080"
echo "ArgoCD User: admin"
echo "ArgoCD Pass: ${ARGOCD_PASS}"

#------------------------------------------------------------------------------
# [i] Register clusters to ArgoCD
#------------------------------------------------------------------------------

blue "───[ Register clusters to ArgoCD ]──────────────────────────────────────"

# Check if the ArgoCD context exists
argocd context "${MNGR}" 2> /dev/null || {

  # Get the external LoadBalancer IP
  ARGOCD_IP=$(getExtIP "${MNGR}" argocd argocd-server)

  # Login to ArgoCD
  argocd login "${ARGOCD_IP}" \
    --insecure \
    --name "${MNGR}" \
    --username admin \
    --password "${ARGOCD_PASS}"

  # Add clusters with labels
  for CELL in $(list cells wkld); do for CLUSTER in ${CELLS[${CELL}]}; do
    argocd cluster list | grep -q "${CLUSTER}" || \
    argocd cluster add -y "kind-${CLUSTER}" \
      --name "${CLUSTER}" \
      --label name="${CLUSTER}" \
      --label cell="${CELL}"
  done; done
}

#------------------------------------------------------------------------------
# [i] Install all the ApplicationSets
#------------------------------------------------------------------------------

blue "───[ ApplicationSets ]──────────────────────────────────────────────────"

helm template ./charts/cilium --set "chartVersion=${CILIUM_CHART_VERSION}" | k0 apply -f -
helm template ./charts/prometheus --set "chartVersion=${PROMETHEUS_CHART_VERSION}" | k0 apply -f -
helm template ./charts/grafana --set "chartVersion=${GRAFANA_CHART_VERSION}" | k0 apply -f -
helm template ./charts/otel-collector --set "chartVersion=${OTEL_COLLECTOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/vault --set "chartVersion=${VAULT_CHART_VERSION}" | k0 apply -f -
helm template ./charts/cert-manager --set "chartVersion=${CERT_MANAGER_CHART_VERSION}" | k0 apply -f -
helm template ./charts/kubernetes-replicator --set "chartVersion=${KUBERNETES_REPLICATOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/istio --set "chartVersion=${ISTIO_CHART_VERSION}" | k0 apply -f -

#------------------------------------------------------------------------------
# [i] Setup ArgoWF
#------------------------------------------------------------------------------

blue "───[ ArgoWF ]───────────────────────────────────────────────────────────"

# Install Argo Workflows
h0 upgrade --install -n argowf --create-namespace --wait --timeout 5m \
  argo-workflows argo/argo-workflows --version "${ARGOWF_CHART_VERSION}" \
  --set 'server.serviceType=LoadBalancer' \
  --set 'server.authModes={server}' \
  --set 'server.servicePort=80' \
  --set 'workflow.serviceAccount.create=true' \
  --set 'workflow.serviceAccount.name=argo-workflow' \
  --set 'workflow.rbac.create=true' \
  --set 'controller.workflowNamespaces={argocd}' |
  grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'

# Set argowf.${DOMAIN} as ext DNS name
k0 -n argowf annotate svc argo-workflows-server coredns.io/hostname=argowf."${DOMAIN}"

# Patch the argo-workflow-role to allow patching of the ApplicationSets
k0 -n argocd patch role argo-workflows-workflow --type json \
  -p '[
        {
          "op": "add",
          "path": "/rules/-",
          "value": {
            "apiGroups":["argoproj.io"],
            "resources":["applicationsets"],
            "verbs":["get","watch","patch"]
          }
        }
      ]'

# Publish the ArgoWF port
publish "${MNGR}" argowf argo-workflows-server 8081

echo
echo "ArgoWF WebUI: http://$(hostname -I | awk '{print $2}'):8081"

#------------------------------------------------------------------------------
# [i] Bootstrap DAG
#------------------------------------------------------------------------------

blue "───[ Bootstrap DAG ]────────────────────────────────────────────────────"

# Install Argo WorkflowTemplates
helm template ./charts/wftemplates | k0 -n argocd apply -f -

# Create a secret with ArgoCD credentials
k0 create secret generic argocd-credentials \
 --from-literal=password="${ARGOCD_PASS}" \
 --from-literal=username='admin' \
 --from-literal=token='' \
 --dry-run=client -o yaml | k0 -n argocd apply -f -

# Create the monitoring namespace on all clusters
for CLUSTER in $(list clusters); do (
  kubectl --context "kind-${CLUSTER}" create ns monitoring \
  --dry-run=client -o yaml | kubectl --context "kind-${CLUSTER}" apply -f -
) & done; wait

# Create the bootstrap workflow
argo --context "kind-${MNGR}" list -n argocd | grep -q bootstrap || \
argo --context "kind-${MNGR}" submit -n argocd -w - << EOF
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: bootstrap-
spec:
  entrypoint: stages
  serviceAccountName: argo-workflow
  templates:
  - name: stages
    dag:
      tasks:
      - name: cilium
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cilium
      - name: prometheus
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=prometheus
      - name: grafana
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=grafana
      - name: otelco-node
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-node
      - name: otelco-cluster
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-cluster
      - name: vault
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=vault
      - name: cert-manager
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cert-manager
      - name: kubernetes-replicator
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=kubernetes-replicator
      - name: populate-vault
        dependencies: [vault]
        templateRef:
          name: populate-vault
          template: populate-vault
      - name: istio-issuers
        dependencies: [populate-vault, cert-manager]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-issuers
      - name: istio-base
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-base
      - name: istio-cni
        dependencies: [istio-base]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-cni
      - name: istio-istiod
        dependencies: [istio-base, istio-issuers]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-istiod
      - name: istio-ztunnel
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ztunnel
      - name: istio-nsgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-nsgw
      - name: istio-ewgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ewgw
EOF

#------------------------------------------------------------------------------
# [i] Enable Istio endpoint discovery
#------------------------------------------------------------------------------

blue "───[ Istio remote secrets ]─────────────────────────────────────────────"

# Bug? Patch the Istio reader ClusterRole.
# TODO: Remove this if fixed by upstream.
# https://github.com/istio/istio/issues/52739
# https://github.com/istio/istio/issues/42137
for CLUSTER in $(list clusters wkld); do

  # Continue if already patched
  k --context "kind-${CLUSTER}" get clusterrole \
  "istio-reader-clusterrole-${ISTIO_CHART_VERSION//./-}-istio-system" \
  -o jsonpath='{.rules[1].resources}' | grep -q configmaps && continue

  # Patch
  k --context "kind-${CLUSTER}" patch clusterrole \
  "istio-reader-clusterrole-${ISTIO_CHART_VERSION//./-}-istio-system" \
  --type='json' -p='[{"op": "add", "path": "/rules/1/resources/-", "value": "configmaps"}]'
done

for CELL in $(list cells wkld); do (
  IFS=" " read -r -a CLUSTERS <<< "${CELLS[${CELL}]}"
  istioctl create-remote-secret --context "kind-${CLUSTERS[0]}" --name="${CLUSTERS[0]}" | \
  kubectl --context "kind-${CLUSTERS[1]}" apply -f -
  istioctl create-remote-secret --context "kind-${CLUSTERS[1]}" --name="${CLUSTERS[1]}" | \
  kubectl --context "kind-${CLUSTERS[0]}" apply -f -
) & done; wait

#------------------------------------------------------------------------------
# [i] Import the meshlab Grafana dashboard
#------------------------------------------------------------------------------

blue "───[ Import Grafana dashboard ]─────────────────────────────────────────"

GRAFANA_IP=$(
  k0 -n monitoring get svc grafana -o yaml |
  yq '.status.loadBalancer.ingress[0].ip'
)

GRAFANA_PORT=$(
  k0 -n monitoring get svc grafana -o yaml |
  yq '.spec.ports[0].port'
)

curl -sX POST \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -H 'Content-Type: application/json' \
  --data @conf/dashboard.json \
  "http://${GRAFANA_IP}:${GRAFANA_PORT}/api/dashboards/db" | jq

#------------------------------------------------------------------------------
# [i] Publish ports
#------------------------------------------------------------------------------

publish "${MNGR}" vault vault 8082
publish "${MNGR}" monitoring prometheus-server 8083
publish "${MNGR}" monitoring grafana 8084

#------------------------------------------------------------------------------
# [i] Echo useful info
#------------------------------------------------------------------------------

blue "───[ Info ]─────────────────────────────────────────────────────────────"

echo "┌─────────────────────────────────────────────────┬────────────────────────────────────────────────────────────┬──────────────┬──────────┬──────────┐"
echo "│ Port mapping                                    │ External                                                   │   Cluster    │          │          │"
echo "├────────────────────────┬────────────────────────┼────────────────────┬───────────────────────────────────────┼────┬─────────┤ Username │ Password │"
echo "│ Envoy                  │ Service                │ IP                 │ Service                               │ IP │ Service │          │          │"

# shellcheck disable=SC2046
( docker inspect $(docker ps -f "name=kindccm-" -q) | jq -r \
  --arg domain "${DOMAIN}" \
  --arg mngr "${MNGR}" '.[] |
  select(.Config.Labels."io.x-k8s.cloud-provider-kind.cluster" == "\($mngr)") |
  (
    .NetworkSettings.Ports
    | map_values(.[] | .HostPort)
    | to_entries
    | map("http://127.0.0.1:" + .value)
    | .[:2]
    | join(" │ ")
  ) as $ports |
  .NetworkSettings.Networks.kind.IPAddress as $ip |
  .Config.Labels."io.x-k8s.cloud-provider-kind.loadbalancer.name" as $lb_name | "
├────────────────────────┼────────────────────────┼────────────────────┼───────────────────────────────────────┤
│ \($ports) │ http://\($ip) │ http://\($lb_name | split("/")[-1]).\($domain)      │"
' ) | column -t -s '|'
