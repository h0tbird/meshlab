#!/usr/bin/env bash

#------------------------------------------------------------------------------
# [i] Initializations
#------------------------------------------------------------------------------

# Bash strict mode
set -euo pipefail

# Change to the execution directory
cd "$(dirname "$0")"/..

# shellcheck source=/dev/null
source lib/common.sh

#------------------------------------------------------------------------------
# [i] Versions
#------------------------------------------------------------------------------

CILIUM_CHART_VERSION='1.16.5'                 # https://artifacthub.io/packages/helm/cilium/cilium
K8S_GATEWAY_CHART_VERSION='2.4.0'             # https://github.com/ori-edge/k8s_gateway/tree/master/charts
ARGOCD_CHART_VERSION='7.7.10'                 # https://artifacthub.io/packages/helm/argo-cd-oci/argo-cd
ARGOWF_CHART_VERSION='0.45.2'                 # https://artifacthub.io/packages/helm/argo/argo-workflows
PROMETHEUS_CHART_VERSION='26.0.1'             # https://artifacthub.io/packages/helm/prometheus-community/prometheus
GRAFANA_CHART_VERSION='8.8.2'                 # https://artifacthub.io/packages/helm/grafana/grafana
OTEL_COLLECTOR_CHART_VERSION='0.111.0'        # https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-collector
VAULT_CHART_VERSION='0.29.1'                  # https://artifacthub.io/packages/helm/hashicorp/vault
CERT_MANAGER_CHART_VERSION='v1.16.2'          # https://artifacthub.io/packages/helm/cert-manager/cert-manager
KUBERNETES_REPLICATOR_CHART_VERSION='2.11.0'  # https://artifacthub.io/packages/helm/kubernetes-replicator/kubernetes-replicator
ISTIO_CHART_VERSION='1.24.2'                  # https://artifacthub.io/packages/helm/istio-official/base

#------------------------------------------------------------------------------
# [i] Validate the command line arguments
#------------------------------------------------------------------------------

[[ $# -ne 1 || ($1 != "create" && $1 != "delete") ]] && \
{ echo "Usage: $0 <create|delete>"; exit 1; }

#------------------------------------------------------------------------------
# [i] Handle the delete command
#------------------------------------------------------------------------------

if [[ $1 == "delete" ]]; then
  # shellcheck disable=SC2046
  docker kill $(docker ps -f "name=kindccm" -q) 2>/dev/null || true
  argocd context --delete "${MNGR}" 2>/dev/null || true
  kind delete clusters --all
  rm -rf ./tmp/*
  exit 0
fi

#------------------------------------------------------------------------------
# [i] cloud-provider-kind
#------------------------------------------------------------------------------

blue "---[ cloud-provider-kind ]----------------------------------------------"

docker network create kind --subnet 172.18.0.0/16 2>/dev/null || true

docker ps | grep -q kindccm || {
  docker run --rm -d \
    --name kindccm \
    --network kind \
    --volume /var/run/docker.sock:/var/run/docker.sock \
    registry.k8s.io/cloud-provider-kind/cloud-controller-manager:v0.4.0 \
    --enable-lb-port-mapping=true
}

#------------------------------------------------------------------------------
# [i] Pull-through image cache
#------------------------------------------------------------------------------

blue "---[ Pull-through registries ]------------------------------------------"

docker start registry-docker.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
  --restart always \
  --name registry-docker.io \
  --network kind \
  registry:2

docker start registry-quay.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://quay.io \
  --restart always \
  --name registry-quay.io \
  --network kind \
  registry:2

docker start registry-ghcr.io 2>/dev/null || docker run -d \
  -e REGISTRY_PROXY_REMOTEURL=https://ghcr.io \
  --restart always \
  --name registry-ghcr.io \
  --network kind \
  registry:2

#------------------------------------------------------------------------------
# [i] Create the clusters
#------------------------------------------------------------------------------

blue "---[ KinD clusters ]----------------------------------------------------"

# Create the temporary directory
[ -d ./tmp ] || mkdir -p ./tmp

# Configure kind
cat << EOF > ./tmp/kind.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  disableDefaultCNI: true
  kubeProxyMode: "none"
containerdConfigPatches:
  - |-
    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
      endpoint = ["http://registry-docker.io:5000"]
    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]
      endpoint = ["http://registry-quay.io:5000"]
    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."ghcr.io"]
      endpoint = ["http://registry-ghcr.io:5000"]
EOF

# Create the clusters
for CLUSTER in $(list clusters all); do
  if ! kind get clusters 2>/dev/null | grep -q "${CLUSTER}"; then
    ( kind create cluster --name "${CLUSTER}" --config ./tmp/kind.yaml -q ) &
  fi
done; wait

# Get all the IPs
declare -A IP
for CLUSTER in $(list clusters all); do
  IP[${CLUSTER}]=$(docker inspect "${CLUSTER}-control-plane" |
    jq -r '.[].NetworkSettings.Networks.kind.IPAddress')
  echo "${CLUSTER} = ${IP[${CLUSTER}]}"
done

#------------------------------------------------------------------------------
# [i] Setup KUBECONFIG
#------------------------------------------------------------------------------

blue "---[ KUBECONFIG ]-------------------------------------------------------"

# Use routable IPs
for CLUSTER in $(list clusters all); do
  SERVER="https://${IP[${CLUSTER}]}:6443"; echo "Server: ${SERVER}"
  kubectl config set-cluster "kind-${CLUSTER}" --server="${SERVER}"
done

function k0 {
  kubectl --context kind-kube-00 "${@}"
}

function h0 {
  helm --kube-context kind-kube-00 "${@}"
}

#------------------------------------------------------------------------------
# [i] Helm repositories
#------------------------------------------------------------------------------

blue "---[ Helm repos ]-------------------------------------------------------"

helm repo add cilium https://helm.cilium.io/
helm repo add argo https://argoproj.github.io/argo-helm
helm repo add k8s_gateway https://ori-edge.github.io/k8s_gateway
helm repo update

#------------------------------------------------------------------------------
# [i] Install the Cilium CNI
#------------------------------------------------------------------------------

blue "---[ Cilium CNI ]--------------------------------------------------------"

# Install the Cilium CNI
for CLUSTER in $(list clusters all); do
  { helm --kube-context "kind-${CLUSTER}" \
      upgrade -i cilium cilium/cilium \
      --version "${CILIUM_CHART_VERSION}" \
      --namespace kube-system \
      --values ./charts/cilium/values.yaml \
      --values ./charts/cilium/values/"${CLUSTER}".yaml \
      --set k8sServiceHost="${IP[${CLUSTER}]}" |
    grep 'LAST DEPLOYED'; } &
done; wait

echo

for CLUSTER in $(list clusters all); do
  { kubectl --context "kind-${CLUSTER}" \
      wait --for=condition=Ready nodes \
      --all --timeout=120s; } &
done; wait

#------------------------------------------------------------------------------
# [i] Install ClusterMesh
#------------------------------------------------------------------------------

blue "---[ ClusterMesh ]------------------------------------------------------"

# With --service-type LoadBalancer, cilium needs connectivity to the external IPs
# Works if it runns on the linux VM, but not on OSX without a route and a TCP proxy.

for CELL in $(list cells); do
  CLUS=(${CELLS[${CELL}]})
  cilium clustermesh enable --context "kind-${CLUS[0]}" --service-type LoadBalancer
  cilium clustermesh enable --context "kind-${CLUS[1]}" --service-type LoadBalancer
  cilium clustermesh connect --context "kind-${CLUS[0]}" --destination-context "kind-${CLUS[1]}"
done

#------------------------------------------------------------------------------
# [i] Install k8s_gateway
#------------------------------------------------------------------------------

blue "---[ k8s_gateway ]------------------------------------------------------"

for CLUSTER in $(list clusters all); do
  { helm --kube-context "kind-${CLUSTER}" \
      upgrade -i exdns k8s_gateway/k8s-gateway \
      --version "${K8S_GATEWAY_CHART_VERSION}" \
      --namespace kube-system \
      --set domain=demo.lab |
    grep 'LAST DEPLOYED'; } &
done; wait

#------------------------------------------------------------------------------
# [i] Setup internal CoreDNS for the demo.lab domain
#------------------------------------------------------------------------------

blue "---[ CoreDNS ]----------------------------------------------------------"

MNGR_EXDNS_IP=$(k0 -n kube-system get svc exdns-k8s-gateway -o yaml | yq '.status.loadBalancer.ingress[0].ip')

for CLUSTER in $(list clusters all); do

  kubectl --context "kind-${CLUSTER}" -n kube-system create cm coredns \
  --dry-run=client -o yaml --from-literal=Corefile="
  .:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30 {
       disable success cluster.local
       disable denial cluster.local
    }
    loop
    reload
    loadbalance
  }
  demo.lab:53 {
    forward . ${MNGR_EXDNS_IP}
  }" | kubectl --context "kind-${CLUSTER}" -n kube-system apply -f -

  # Restart otherwise it takes a while to pick up the new config
  kubectl --context "kind-${CLUSTER}" -n kube-system rollout restart deployment coredns

done

#------------------------------------------------------------------------------
# [i] Setup ArgoCD
#------------------------------------------------------------------------------

blue "---[ ArgoCD ]-----------------------------------------------------------"

# Install ArgoCD
h0 upgrade --install -n argocd --create-namespace --wait --timeout 5m \
argocd argo/argo-cd --version "${ARGOCD_CHART_VERSION}" \
--set 'server.service.type=LoadBalancer' \
-f - << EOF | grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'
configs:
  cm:
    resource.customizations.ignoreDifferences.admissionregistration.k8s.io_MutatingWebhookConfiguration: |
      jqPathExpressions:
      - '.webhooks[]?.clientConfig.caBundle'
EOF

# Set argocd.demo.lab as ext DNS name
k0 -n argocd annotate svc argocd-server coredns.io/hostname=argocd.demo.lab

# Get the password
ARGOCD_PASS=$(
  k0 -n argocd \
  get secret argocd-initial-admin-secret \
  -o jsonpath='{.data.password}' | base64 -d
)

# Get the external LoadBalancer IP
ARGOCD_IP=$(
  k0 -n argocd \
  get svc argocd-server \
  -o yaml | yq '.status.loadBalancer.ingress[0].ip'
)

echo
echo "ArgoCD WebUI: https://${ARGOCD_IP}"
echo "ArgoCD User: admin"
echo "ArgoCD Pass: ${ARGOCD_PASS}"

#------------------------------------------------------------------------------
# [i] Register clusters to ArgoCD
#------------------------------------------------------------------------------

blue "---[ Register clusters to ArgoCD ]--------------------------------------"

# Check if the ArgoCD context exists
argocd context "${MNGR}" 2> /dev/null || {

  # Login to ArgoCD
  argocd login "${ARGOCD_IP}" \
    --insecure \
    --name "${MNGR}" \
    --username admin \
    --password "${ARGOCD_PASS}"

  # Add clusters with labels
  for CELL in $(list cells); do for CLUSTER in ${CELLS[${CELL}]}; do
    argocd cluster list | grep -q "${CLUSTER}" || \
    argocd cluster add -y "kind-${CLUSTER}" \
      --name "${CLUSTER}" \
      --label name="${CLUSTER}" \
      --label cell="${CELL}"
  done; done
}

#------------------------------------------------------------------------------
# [i] Install all the ApplicationSets
#------------------------------------------------------------------------------

blue "---[ ApplicationSets ]-------------------------------------------------"

helm template ./charts/cilium --set "chartVersion=${CILIUM_CHART_VERSION}" | k0 apply -f -
helm template ./charts/prometheus --set "chartVersion=${PROMETHEUS_CHART_VERSION}" | k0 apply -f -
helm template ./charts/grafana --set "chartVersion=${GRAFANA_CHART_VERSION}" | k0 apply -f -
helm template ./charts/otel-collector --set "chartVersion=${OTEL_COLLECTOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/vault --set "chartVersion=${VAULT_CHART_VERSION}" | k0 apply -f -
helm template ./charts/cert-manager --set "chartVersion=${CERT_MANAGER_CHART_VERSION}" | k0 apply -f -
helm template ./charts/kubernetes-replicator --set "chartVersion=${KUBERNETES_REPLICATOR_CHART_VERSION}" | k0 apply -f -
helm template ./charts/istio --set "chartVersion=${ISTIO_CHART_VERSION}" | k0 apply -f -

#------------------------------------------------------------------------------
# [i] Setup ArgoWF
#------------------------------------------------------------------------------

blue "---[ ArgoWF ]-----------------------------------------------------------"

# Install Argo Workflows
h0 upgrade --install -n argowf --create-namespace --wait --timeout 5m \
  argo-workflows argo/argo-workflows --version "${ARGOWF_CHART_VERSION}" \
  --set 'server.serviceType=LoadBalancer' \
  --set 'server.authModes={server}' \
  --set 'server.servicePort=80' \
  --set 'workflow.serviceAccount.create=true' \
  --set 'workflow.serviceAccount.name=argo-workflow' \
  --set 'workflow.rbac.create=true' \
  --set 'controller.workflowNamespaces={argocd}' |
  grep -E '^NAME|^LAST|^STATUS|^REVISION|^TEST'

# Set argowf.demo.lab as ext DNS name
k0 -n argowf annotate svc argo-workflows-server coredns.io/hostname=argowf.demo.lab

# Patch the argo-workflow-role to allow patching of the ApplicationSets
k0 -n argocd patch role argo-workflows-workflow --type json \
  -p '[
        {
          "op": "add",
          "path": "/rules/-",
          "value": {
            "apiGroups":["argoproj.io"],
            "resources":["applicationsets"],
            "verbs":["get","watch","patch"]
          }
        }
      ]'

# Get the external LoadBalancer IP
ARGOWF_IP=$(
  k0 -n argowf \
  get svc argo-workflows-server \
  -o yaml | yq '.status.loadBalancer.ingress[0].ip'
)

echo
echo "ArgoWF WebUI: http://${ARGOWF_IP}"

#------------------------------------------------------------------------------
# [i] Bootstrap DAG
#------------------------------------------------------------------------------

blue "---[ Bootstrap DAG ]----------------------------------------------------"

# Install Argo WorkflowTemplates
helm template ./charts/wftemplates | k0 -n argocd apply -f -

# Create a secret with ArgoCD credentials
k0 create secret generic argocd-credentials \
 --from-literal=password="${ARGOCD_PASS}" \
 --from-literal=username='admin' \
 --from-literal=token='' \
 --dry-run=client -o yaml | k0 -n argocd apply -f -

# Create the bootstrap workflow
argo --context kind-kube-00 list -n argocd | grep -q bootstrap || \
argo --context kind-kube-00 submit -n argocd -w - << EOF
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: bootstrap-
spec:
  entrypoint: stages
  serviceAccountName: argo-workflow
  templates:
  - name: stages
    dag:
      tasks:
      - name: cilium
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cilium
      - name: prometheus
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=prometheus
      - name: grafana
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=grafana
      - name: otelco-node
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-node
      - name: otelco-cluster
        dependencies: [prometheus]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=otelco-cluster
      - name: vault
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=vault
      - name: cert-manager
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=cert-manager
      - name: kubernetes-replicator
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=kubernetes-replicator
      - name: populate-vault
        dependencies: [vault]
        templateRef:
          name: populate-vault
          template: populate-vault
      - name: istio-issuers
        dependencies: [populate-vault, cert-manager]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-issuers
      - name: istio-base
        dependencies: [cilium]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-base
      - name: istio-cni
        dependencies: [istio-base]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-cni
      - name: istio-istiod
        dependencies: [istio-base, istio-issuers]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-istiod
      - name: istio-ztunnel
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ztunnel
      - name: istio-nsgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-nsgw
      - name: istio-ewgw
        dependencies: [istio-istiod]
        templateRef:
          name: argocd-sync-and-wait
          template: argocd-sync-and-wait
        arguments:
          parameters:
          - name: selectors
            value: name=istio-ewgw
EOF
